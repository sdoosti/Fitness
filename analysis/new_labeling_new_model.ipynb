{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Labeling by Document Similarity\n",
    "We take a different approach to label the comments. In this approach, we use labeled comments and find similar comments to label them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\doosti\\.conda\\envs\\top2vec\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from top2vec import Top2Vec\n",
    "import sys\n",
    "import re\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the total number of comments is 830579\n",
      "the total number of comments processed is 414616\n"
     ]
    }
   ],
   "source": [
    "\n",
    "DATA_PATH = \"C:/Users/doosti/Dropbox (Chapman)/Research/Research Projects/Fitness/Data/\"\n",
    "processed_file=\"processed_comments_122923.txt\"\n",
    "# use processed_comments_122923_fast.txt if newpreprocessed model is used (instead of newpreprocessed2)\n",
    "comments_file=\"merged_comments.csv\"\n",
    "#labeled = \"comments_activity_motives.csv\"\n",
    "#sim_labels = \"comments_similarity_labels.csv\"\n",
    "labeled = \"comments_similarity_labels.csv\"\n",
    "\n",
    "labeled = pd.read_csv(os.path.join(DATA_PATH, labeled))\n",
    "#sim_labels = pd.read_csv(os.path.join(DATA_PATH, sim_labels))\n",
    "comment_length=10\n",
    "with open(os.path.join(DATA_PATH,processed_file),\"r\", encoding=\"utf-8\") as f:\n",
    "    processed_docs = f.readlines()\n",
    "length_include = [len(re.sub(\"\\d+\", \"\", x.strip()).split(','))>5 for x in processed_docs]\n",
    "\n",
    "comments = pd.read_csv(os.path.join(DATA_PATH, \"merged_comments.csv\"))\n",
    "#comments = comments[comments.comment_text.notnull()].copy()\n",
    "\n",
    "print(f\"the total number of comments is {comments.shape[0]+labeled.shape[0]}\")\n",
    "\n",
    "comments2 = pd.DataFrame(data={'comment_text':comments.comment_text.tolist()+labeled.comment_text.tolist()})\n",
    "comments2 = comments2[comments2.comment_text.notnull()].copy()\n",
    "comments2['include'] = length_include\n",
    "comments2['processed'] = processed_docs\n",
    "print(f\"the total number of comments processed is {comments2[comments2.include].shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"top2vec_lowercase_learn_doc2vec.model\"\n",
    "# model_name = \"top2vec_lowercase_newpreprocessed_deep-learn_universal-sentence-encoder.model\"\n",
    "model_name = \"top2vec_lowercase_newpreprocessed2_deep-learn_universal-sentence-encoder-multilingual-large.model\"\n",
    "# model_name = \"top2vec_lowercase_newpreprocessed2_deep-learn_doc2vec.model\"\n",
    "model = Top2Vec.load(f\"E:/{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([     0,      1,      2, ..., 414613, 414614, 414615], dtype=int64)"
      ]
     },
     "execution_count": 410,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.document_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the model with queries\n",
    "queries = {}\n",
    "queries['query1'] = \"I want to lose weight\"\n",
    "queries['query2'] = \"I want to be healthy\"\n",
    "queries['query3'] = \"I love the yoga community\"\n",
    "queries['query4'] = \"I want to be fit\"\n",
    "queries['query5'] = \"I want to ease my anxiety\"\n",
    "queries['query6'] = \"I want to look good\"\n",
    "queries['query7'] = \"I love doing Yoga with other people\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: top2vec_lowercase_newpreprocessed2_deep-learn_universal-sentence-encoder-multilingual-large.model\n",
      "query: I want to lose weight\n",
      "['want lose weight plz help sir' 'want lose kg lose kg help'\n",
      " 'want lose weight low body plz help' 'mera weight kg want lose weight'\n",
      " 'hi bro want lose weight m try not m want weight']\n",
      "query: I want to be healthy\n",
      "['want feel good body healthy habit'\n",
      " 'shoutout want good healthy change body live'\n",
      " 'wish healthy life fill health joy light'\n",
      " 'good k wish love health happiness'\n",
      " 'want well physical mental health acceptance']\n",
      "query: I love the yoga community\n",
      "['love yoga yoga luna yoga community thank' 'yay love s yoga s awesome'\n",
      " 'thankful beautiful yoga community awesome got'\n",
      " 'absolute fav love yoga cz thank'\n",
      " 'thank dedication yoga community pleasure practice namaste']\n",
      "query: I want to be fit\n",
      "['want feel good body healthy habit'\n",
      " 'omg want body like haha tell exercise'\n",
      " 'want well posture exercise slouch want exercise'\n",
      " 'want lose weight low body plz help' 'want lose weight plz help sir']\n",
      "query: I want to ease my anxiety\n",
      "['wish stuff like work give anxiety'\n",
      " 'thank need m deal lot stress calming' 'ufff want afraid l badly strain'\n",
      " 'lovely calming need help anxiety thank paula'\n",
      " 'have high anxiety day help bit thank']\n",
      "query: I want to look good\n",
      "['want feel good body healthy habit'\n",
      " 'wish good look beautiful overflow happiness'\n",
      " 'exactly think look bad realize good look'\n",
      " 'want well posture exercise slouch want exercise'\n",
      " 'wanna suck look like gross skeleton lmao']\n",
      "query: I love doing Yoga with other people\n",
      "['love join guy yoga practice thank' 'yay love s yoga s awesome'\n",
      " 'love yoga pilate combo definitely yoga'\n",
      " 'love good busy people love yoga' 'love combo pilates yoga feel good']\n"
     ]
    }
   ],
   "source": [
    "print(f\"model: {model_name}\")\n",
    "for k, query in queries.items():\n",
    "    print(f\"query: {query}\")\n",
    "    docus, scores, doc_ids = model.query_documents(query, num_docs=5)\n",
    "    print(docus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document ID conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        comment_text  include  \\\n",
      "0  I loved doing this video with my dad I think i...     True   \n",
      "1             bre is very good at yoga same with Flo    False   \n",
      "2  Of course I can add 15 more minutes to round o...     True   \n",
      "3  Thanks for helping round out my routine, was q...     True   \n",
      "4          First thing tomorrow morning! Thank you üôè    False   \n",
      "\n",
      "                                           processed  doc_id  \n",
      "0                    loved,video,dad,think,th,good\\n     0.0  \n",
      "1                                bre,good,yoga,flo\\n     NaN  \n",
      "2  course,add,minute,round,morning,practice,pulse...     1.0  \n",
      "3   thank,help,round,routine,stagnant,find,channel\\n     2.0  \n",
      "4                     thing,tomorrow,morning,thank\\n     NaN  \n"
     ]
    }
   ],
   "source": [
    "# converting the old index to the new index\n",
    "comments2['doc_id'] = np.NaN\n",
    "comments2.loc[comments2.include,'doc_id'] = np.arange(comments2.include.sum())\n",
    "print(comments2.head())\n",
    "comments2.doc_id.describe()\n",
    "id_conv = comments2.doc_id.to_dict()\n",
    "for k,v in id_conv.items():\n",
    "    if np.isnan(v):\n",
    "        id_conv[k] = None\n",
    "    else:\n",
    "        id_conv[k] = int(v)\n",
    "\n",
    "# convert an array of ids to the new index (if the id is not in the new index, it will be None)\n",
    "def convert_id_l2s(x):\n",
    "    # Long version to short version\n",
    "    return [id_conv.get(i) for i in x if id_conv.get(i) is not None]\n",
    "\n",
    "# convert new ids to old ids\n",
    "id_conv2 = comments2[comments2.include].doc_id.to_dict()\n",
    "id_conv2 = {int(v):k for k,v in id_conv2.items() if v is not None}\n",
    "\n",
    "def convert_id_s2l(x):\n",
    "    # Short version to long version\n",
    "    return [id_conv2.get(i) for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3]\n",
      "[0, 2, 3, 7, 12, 16, 17, 19, 25, 27, 29]\n"
     ]
    }
   ],
   "source": [
    "# test the conversion\n",
    "print(convert_id_l2s([0,1,2,3,4,5,6,7,8,9,10]))\n",
    "print(convert_id_s2l([0,1,2,3,4,5,6,7,8,9,10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label using survey documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "Enjoyment = [\"it is fun\", \"i like to do yoga\", \"it makes me happy\", \"it is interesting\", \"i enjoy yoga\", \"i find yoga stimulating\", \"i like the excitement of participation\"]\n",
    "Competence = [\"i like engaging in yoga which physically challenge me\", \"i want to obtain new skills\", \"i want to improve existing skills\", \"i like the challenge\", \"i want to keep up my current skill level\", \"i like activities which are physically challenging to me\", \"i want to get better at yoga\"]\n",
    "Appearance = [\"i want to look better\", \"i want to improve my appearance\", \"i want to be attractive\", \"i want to improve my body shape\"] #\"i feel physically unattractive if i do not do yoga\"\n",
    "Fitness = [\"i want to be physically fit\", \"i want to have more energy\", \"i want to improve my cardiovascular fitness\", \"i want to maintain my physicall strength to live a healthy life\", \"i want to maintain my physical health and well-being\"]\n",
    "Social = [\"i love the community\", \"i want to be with my friends\", \"i like to be with others who are interested in this activity\", \"i want to meet new people\", \"my friends want me to\", \"i enjoy spending time with others doing yoga\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(queries, comment_id = True):\n",
    "    \"\"\"\n",
    "    This function takes a list of queries and returns a dictionary of scores for each query\n",
    "    Parameters:\n",
    "    queries (list): a list of queries\n",
    "\n",
    "    comment_id (bool): if True, the keys of the dictionary are comment ids, otherwise they are model doc ids\n",
    "\n",
    "    Returns:\n",
    "    scores (dict): a dictionary of average scores for each query with ids as keys\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    num_queries = len(queries)\n",
    "    for query in queries:\n",
    "        docus, scores, doc_ids = model.query_documents(query, num_docs=100000, ef=400000)\n",
    "        scores_dict = {}\n",
    "        if comment_id:\n",
    "            doc_ids = convert_id_s2l(doc_ids)\n",
    "        for score, doc in zip(scores, doc_ids):\n",
    "            if doc in scores:\n",
    "                scores_dict[doc] += score/num_queries\n",
    "            else:\n",
    "                scores_dict[doc] = score/num_queries\n",
    "    return scores_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scores for each label\n",
    "label_scores = {}\n",
    "label_scores['Enjoyment'] = get_scores(Enjoyment)\n",
    "label_scores['Competence'] = get_scores(Competence)\n",
    "label_scores['Appearance'] = get_scores(Appearance)\n",
    "label_scores['Fitness'] = get_scores(Fitness)\n",
    "label_scores['Social'] = get_scores(Social)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enjoyment 100000\n",
      "Competence 100000\n",
      "Appearance 100000\n",
      "Fitness 100000\n",
      "Social 100000\n",
      "          Competence        Fitness     Appearance      Enjoyment  \\\n",
      "count  830479.000000  830479.000000  830479.000000  830479.000000   \n",
      "mean        0.006620       0.005541       0.007764       0.004531   \n",
      "std         0.018354       0.015347       0.021463       0.012433   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000       0.000000       0.000000       0.000000   \n",
      "50%         0.000000       0.000000       0.000000       0.000000   \n",
      "75%         0.000000       0.000000       0.000000       0.000000   \n",
      "max         0.113910       0.146740       0.142680       0.089487   \n",
      "\n",
      "              Social  \n",
      "count  830479.000000  \n",
      "mean        0.007589  \n",
      "std         0.021007  \n",
      "min         0.000000  \n",
      "25%         0.000000  \n",
      "50%         0.000000  \n",
      "75%         0.000000  \n",
      "max         0.120060  \n"
     ]
    }
   ],
   "source": [
    "# propgate the scores to the comments\n",
    "comments_df = pd.DataFrame(data={'comment_text': comments.comment_text.tolist() + labeled.comment_text.tolist()})\n",
    "comments_df = comments_df[comments_df.comment_text.notnull()].copy()\n",
    "comments_df['include'] = length_include\n",
    "comments_df['processed'] = processed_docs\n",
    "\n",
    "# fill the column for each label with the similarity scores using the doc ids\n",
    "for label, scores in label_scores.items():\n",
    "    print(label, len(scores))\n",
    "    comments_df[label] = 0.0\n",
    "    comments_df.loc[list(scores.keys()), label] = list(scores.values())\n",
    "    \n",
    "main_cols = ['Competence', 'Fitness', 'Appearance', 'Enjoyment', 'Social']\n",
    "\n",
    "# get the max score for each comment\n",
    "comments_df['max_score'] = comments_df[main_cols].max(axis=1)\n",
    "# get the label for each comment\n",
    "comments_df['label'] = comments_df[main_cols ].idxmax(axis=1)\n",
    "\n",
    "pprint(comments_df[main_cols].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "Appearance    9569\n",
      "Social        8074\n",
      "Fitness        800\n",
      "Competence     558\n",
      "Enjoyment        4\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cut_off = 0.08\n",
    "#comments_df.loc[comments_df.max_score < cut_off, 'label'] = 'other'\n",
    "pprint(comments_df[comments_df.max_score > cut_off].label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appearance (0.49):\n",
      "'Never seen abs shaped like that. How did the gap in between come there'\n",
      "------------------\n",
      "Appearance (0.45):\n",
      "'what beautiful graceful forms and shapes. absolutely lovely!'\n",
      "------------------\n",
      "Appearance (0.44):\n",
      "'I am so out of shape, lol.. I did it but it was hard! I love it... thank ypu'\n",
      "------------------\n",
      "Appearance (0.43):\n",
      "'A M A Z I N G Allie, more like these please! This was everything and more.'\n",
      "------------------\n",
      "Appearance (0.42):\n",
      "('you look so much better in 2022. Still very slim and much more toned but not '\n",
      " 'way too thin.')\n",
      "------------------\n",
      "Appearance (0.42):\n",
      "('I always missed you.You illustrated very lucidly & charming manners. You are '\n",
      " 'very attractive & lovely.')\n",
      "------------------\n",
      "Appearance (0.41):\n",
      "'Kya sach me breast shape me aa jayega'\n",
      "------------------\n",
      "Appearance (0.41):\n",
      "('Oh I LOVE this!!! I‚Äôve always been an all or nothing person and this is such '\n",
      " 'a beautiful way of looking at it')\n",
      "------------------\n",
      "Appearance (0.40):\n",
      "\"This looks incredible! I'm looking forward to trying it sometime!üëç\"\n",
      "------------------\n",
      "Appearance (0.40):\n",
      "'You are looking especially radiant! Thank you for sharing this joy!'\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "def print_top_label(label,top=20, print_text=True):\n",
    "    for k,row in comments_df[comments_df.label==label].sort_values(by=label, ascending=False).iloc[:top].iterrows():\n",
    "        print(f\"{row.label} ({row[row.label]:.2f}):\")\n",
    "        if print_text:\n",
    "            pprint(row.comment_text)\n",
    "        else:\n",
    "            pprint(row.processed)\n",
    "        print('------------------')\n",
    "\n",
    "# sample social labels\n",
    "label = \"Appearance\"\n",
    "print_top_label(label, top=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documents by Keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = {}\n",
    "keywords['Enjoyment'] = [\"fun\", \"happy\", \"interesting\", \"excitement\"] # \"stimulating\"\n",
    "keywords['Competence'] = [\"challenge\", \"skill\", \"improve\", \"learn\"]\n",
    "keywords['Appearance'] = [\"look\", \"attractive\", \"shape\"]\n",
    "keywords['Fitness'] = [\"fit\", \"energy\", \"cardio\", \"strength\", \"health\"]\n",
    "keywords['Social'] = [\"community\", \"friend\", \"people\"] # \"others\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enjoyment\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "'\n' has not been learned by the model so it cannot be searched.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\doosti\\Dropbox (Chapman)\\Research\\Research Projects\\Fitness\\Fitness\\analysis\\new_labeling_new_model.ipynb Cell 22\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/doosti/Dropbox%20%28Chapman%29/Research/Research%20Projects/Fitness/Fitness/analysis/new_labeling_new_model.ipynb#Y126sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfor\u001b[39;00m label, keys \u001b[39min\u001b[39;00m keywords\u001b[39m.\u001b[39mitems():\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/doosti/Dropbox%20%28Chapman%29/Research/Research%20Projects/Fitness/Fitness/analysis/new_labeling_new_model.ipynb#Y126sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mprint\u001b[39m(label)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/doosti/Dropbox%20%28Chapman%29/Research/Research%20Projects/Fitness/Fitness/analysis/new_labeling_new_model.ipynb#Y126sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     docus, scores, doc_ids \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49msearch_documents_by_keywords(keys, keywords_neg \u001b[39m=\u001b[39;49m [\u001b[39m'\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39m'\u001b[39;49m], num_docs\u001b[39m=\u001b[39;49m\u001b[39m20000\u001b[39;49m, ef\u001b[39m=\u001b[39;49m\u001b[39m400000\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/doosti/Dropbox%20%28Chapman%29/Research/Research%20Projects/Fitness/Fitness/analysis/new_labeling_new_model.ipynb#Y126sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     doc_ids \u001b[39m=\u001b[39m convert_id_s2l(doc_ids)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/doosti/Dropbox%20%28Chapman%29/Research/Research%20Projects/Fitness/Fitness/analysis/new_labeling_new_model.ipynb#Y126sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     key_scores[label] \u001b[39m=\u001b[39m {doc_id:score \u001b[39mfor\u001b[39;00m doc_id, score \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(doc_ids, scores)}\n",
      "File \u001b[1;32mc:\\Users\\doosti\\.conda\\envs\\top2vec\\lib\\site-packages\\top2vec\\Top2Vec.py:2594\u001b[0m, in \u001b[0;36mTop2Vec.search_documents_by_keywords\u001b[1;34m(self, keywords, num_docs, keywords_neg, return_documents, use_index, ef)\u001b[0m\n\u001b[0;32m   2591\u001b[0m     keywords_neg \u001b[39m=\u001b[39m []\n\u001b[0;32m   2593\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_num_docs(num_docs)\n\u001b[1;32m-> 2594\u001b[0m keywords, keywords_neg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_keywords(keywords, keywords_neg)\n\u001b[0;32m   2595\u001b[0m word_vecs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_words2word_vectors(keywords)\n\u001b[0;32m   2596\u001b[0m neg_word_vecs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_words2word_vectors(keywords_neg)\n",
      "File \u001b[1;32mc:\\Users\\doosti\\.conda\\envs\\top2vec\\lib\\site-packages\\top2vec\\Top2Vec.py:1269\u001b[0m, in \u001b[0;36mTop2Vec._validate_keywords\u001b[1;34m(self, keywords, keywords_neg)\u001b[0m\n\u001b[0;32m   1267\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m keywords_lower \u001b[39m+\u001b[39m keywords_neg_lower:\n\u001b[0;32m   1268\u001b[0m     \u001b[39mif\u001b[39;00m word \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m vocab:\n\u001b[1;32m-> 1269\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mword\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m has not been learned by the model so it cannot be searched.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1271\u001b[0m \u001b[39mreturn\u001b[39;00m keywords_lower, keywords_neg_lower\n",
      "\u001b[1;31mValueError\u001b[0m: '\n' has not been learned by the model so it cannot be searched."
     ]
    }
   ],
   "source": [
    "key_scores = {}\n",
    "for label, keys in keywords.items():\n",
    "    print(label)\n",
    "    docus, scores, doc_ids = model.search_documents_by_keywords(keys, keywords_neg = ['\\n'], num_docs=20000, ef=400000)\n",
    "    doc_ids = convert_id_s2l(doc_ids)\n",
    "    key_scores[label] = {doc_id:score for doc_id, score in zip(doc_ids, scores)}\n",
    "    for key in keys:\n",
    "        print(f\"{comments_df[comments_df.comment_text.str.contains(key)].shape[0]} of {key}\")\n",
    "    print('------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enjoyment 20000\n",
      "Competence 20000\n",
      "Appearance 20000\n",
      "Fitness 20000\n",
      "Social 20000\n",
      "          Competence        Fitness     Appearance      Enjoyment  \\\n",
      "count  830479.000000  830479.000000  830479.000000  830479.000000   \n",
      "mean        0.005563       0.006788       0.004899       0.007238   \n",
      "std         0.035785       0.043540       0.031625       0.046490   \n",
      "min         0.000000       0.000000       0.000000       0.000000   \n",
      "25%         0.000000       0.000000       0.000000       0.000000   \n",
      "50%         0.000000       0.000000       0.000000       0.000000   \n",
      "75%         0.000000       0.000000       0.000000       0.000000   \n",
      "max         0.533307       0.581013       0.530676       0.568918   \n",
      "\n",
      "              Social  \n",
      "count  830479.000000  \n",
      "mean        0.004328  \n",
      "std         0.028140  \n",
      "min         0.000000  \n",
      "25%         0.000000  \n",
      "50%         0.000000  \n",
      "75%         0.000000  \n",
      "max         0.529961  \n"
     ]
    }
   ],
   "source": [
    "# propgate the scores to the comments\n",
    "# comments_df = pd.DataFrame(data={'comment_text': comments.comment_text.tolist() + labeled.comment_text.tolist()})\n",
    "# comments_df = comments_df[comments_df.comment_text.notnull()].copy()\n",
    "# comments_df['include'] = length_include\n",
    "# comments_df['processed'] = processed_docs\n",
    "\n",
    "# fill the column for each label with the similarity scores using the doc ids\n",
    "for label, scores in key_scores.items():\n",
    "    print(label, len(scores))\n",
    "    comments_df[label] = 0.0\n",
    "    comments_df.loc[list(scores.keys()), label] = list(scores.values())\n",
    "    \n",
    "main_cols = ['Competence', 'Fitness', 'Appearance', 'Enjoyment', 'Social']\n",
    "\n",
    "# get the max score for each comment\n",
    "comments_df['max_score'] = comments_df[main_cols].max(axis=1)\n",
    "# get the label for each comment\n",
    "comments_df['label'] = comments_df[main_cols ].idxmax(axis=1)\n",
    "\n",
    "pprint(comments_df[main_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "Enjoyment     7308\n",
      "Fitness       4181\n",
      "Competence     623\n",
      "Appearance     231\n",
      "Social         181\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "cut_off = 0.3\n",
    "comments_df.loc[comments_df.max_score < cut_off, 'label'] = 'other'\n",
    "pprint(comments_df[comments_df.max_score > cut_off].label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Social (0.52):\n",
      "'ye,men,kr,skte,h,kya\\n'\n",
      "------------------\n",
      "Social (0.51):\n",
      "'t,h,n,k,y,u\\n'\n",
      "------------------\n",
      "Social (0.47):\n",
      "'t,h,n,k,y,o,u\\n'\n",
      "------------------\n",
      "Social (0.47):\n",
      "'t,h,n,k,y,o,u\\n'\n",
      "------------------\n",
      "Social (0.47):\n",
      "'t,h,n,k,y,o,u\\n'\n",
      "------------------\n",
      "Social (0.47):\n",
      "'t,h,n,k,y,o,u\\n'\n",
      "------------------\n",
      "Social (0.45):\n",
      "'grateful,friend,people,life,kindness,share\\n'\n",
      "------------------\n",
      "Social (0.44):\n",
      "'day,nice,community,feel,like,people\\n'\n",
      "------------------\n",
      "Social (0.44):\n",
      "'nice,community,mean,lot,thank,guy\\n'\n",
      "------------------\n",
      "Social (0.43):\n",
      "'help,lot,person,know,stuff,good\\n'\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "print_top_label('Social', top=10, print_text=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>include</th>\n",
       "      <th>processed</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>299857</th>\n",
       "      <td>T H A N K   Y O U</td>\n",
       "      <td>True</td>\n",
       "      <td>t,h,n,k,y,o,u\\n</td>\n",
       "      <td>163174.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>439128</th>\n",
       "      <td>t h a n k  y o u !!   üåû</td>\n",
       "      <td>True</td>\n",
       "      <td>t,h,n,k,y,o,u\\n</td>\n",
       "      <td>227755.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>526973</th>\n",
       "      <td>T H A N K Y O U üôèüèºü•∞üíú</td>\n",
       "      <td>True</td>\n",
       "      <td>t,h,n,k,y,o,u\\n</td>\n",
       "      <td>274038.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>788706</th>\n",
       "      <td>T h a n k Y o u ü•≤</td>\n",
       "      <td>True</td>\n",
       "      <td>t,h,n,k,y,o,u\\n</td>\n",
       "      <td>394592.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   comment_text  include        processed    doc_id\n",
       "299857        T H A N K   Y O U     True  t,h,n,k,y,o,u\\n  163174.0\n",
       "439128  t h a n k  y o u !!   üåû     True  t,h,n,k,y,o,u\\n  227755.0\n",
       "526973     T H A N K Y O U üôèüèºü•∞üíú     True  t,h,n,k,y,o,u\\n  274038.0\n",
       "788706        T h a n k Y o u ü•≤     True  t,h,n,k,y,o,u\\n  394592.0"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments2[comments2.processed=='t,h,n,k,y,o,u\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_nums, topic_scores, topic_words, word_scores = model.get_documents_topics(np.arange(414542))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "590"
      ]
     },
     "execution_count": 520,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "docus, scores, doc_ids = model.search_documents_by_topic(590,num_docs=109)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words, topic_scores, topic_nums = model.get_topics(591)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['frm', 'ie', 'hm', 'mmm', 'af', 'bs', 'bt', 'sm', 'ei', 'mm', 'nd',\n",
       "       'hiii', 'wt', 'ytt', 'tk', 'ca', 'bta', 'fo', 'jk', 'ai', 'nt',\n",
       "       'aw', 'yt', 'th', 'tks', 'tq', 'thnk', 'hv', 'ce', 'hmmm', 'fwfg',\n",
       "       'rn', 'btao', 'sa', 'ye', 'ho', 'hmm', 'ngl', 'tnx', 'yr', 'bp',\n",
       "       'fr', 'bcz', 'lo', 'aa', 'ta', 'ms', 'bht', 'hr', 'je'],\n",
       "      dtype='<U15')"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_words[590]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([12466,  9047,  8938,  7968,  5904,  5813,  5284,  5280,  5056,\n",
       "         4831,  4729,  4486,  4303,  3621,  3571,  3340,  3039,  3001,\n",
       "         2995,  2972,  2955,  2830,  2800,  2671,  2643,  2395,  2310,\n",
       "         2300,  2294,  2260,  2255,  2237,  2235,  2208,  2142,  2028,\n",
       "         2009,  2004,  1958,  1942,  1931,  1904,  1830,  1724,  1722,\n",
       "         1700,  1691,  1668,  1596,  1579,  1565,  1558,  1546,  1510,\n",
       "         1495,  1485,  1475,  1475,  1445,  1433,  1430,  1400,  1382,\n",
       "         1324,  1320,  1309,  1289,  1282,  1281,  1280,  1228,  1227,\n",
       "         1204,  1196,  1196,  1196,  1187,  1162,  1156,  1154,  1147,\n",
       "         1142,  1138,  1129,  1113,  1091,  1078,  1076,  1072,  1049,\n",
       "         1049,  1035,  1002,   999,   980,   980,   979,   975,   971,\n",
       "          964,   958,   958,   957,   906,   902,   898,   896,   892,\n",
       "          891,   861,   858,   854,   847,   844,   843,   842,   836,\n",
       "          835,   818,   814,   805,   801,   782,   778,   775,   772,\n",
       "          770,   756,   755,   750,   747,   744,   741,   739,   735,\n",
       "          734,   730,   727,   727,   716,   715,   714,   711,   700,\n",
       "          699,   699,   694,   683,   678,   677,   673,   670,   667,\n",
       "          661,   660,   658,   658,   655,   655,   655,   647,   647,\n",
       "          645,   644,   641,   638,   636,   634,   632,   625,   622,\n",
       "          618,   617,   616,   609,   609,   606,   602,   600,   599,\n",
       "          597,   590,   582,   581,   576,   574,   573,   573,   569,\n",
       "          568,   567,   565,   560,   555,   554,   553,   551,   547,\n",
       "          547,   539,   538,   536,   536,   531,   530,   526,   519,\n",
       "          518,   516,   516,   514,   512,   511,   506,   505,   505,\n",
       "          501,   499,   496,   494,   493,   492,   491,   491,   490,\n",
       "          489,   485,   476,   473,   472,   472,   471,   470,   469,\n",
       "          469,   469,   468,   468,   466,   465,   465,   461,   459,\n",
       "          459,   456,   454,   449,   446,   445,   445,   440,   437,\n",
       "          437,   433,   433,   429,   426,   426,   424,   421,   421,\n",
       "          418,   409,   404,   403,   400,   399,   398,   398,   396,\n",
       "          396,   395,   394,   391,   385,   384,   384,   383,   383,\n",
       "          382,   379,   373,   373,   373,   373,   371,   369,   367,\n",
       "          366,   366,   366,   364,   362,   362,   359,   356,   352,\n",
       "          351,   350,   346,   346,   346,   345,   344,   343,   342,\n",
       "          341,   340,   337,   330,   330,   330,   329,   326,   323,\n",
       "          322,   321,   321,   320,   319,   319,   318,   315,   314,\n",
       "          310,   309,   308,   307,   305,   305,   304,   304,   304,\n",
       "          303,   302,   302,   301,   301,   296,   295,   293,   293,\n",
       "          292,   291,   291,   290,   289,   285,   282,   281,   280,\n",
       "          280,   280,   279,   278,   278,   276,   276,   275,   273,\n",
       "          271,   271,   267,   267,   267,   266,   266,   265,   263,\n",
       "          262,   261,   260,   258,   257,   257,   256,   254,   252,\n",
       "          251,   251,   250,   249,   249,   248,   246,   244,   242,\n",
       "          241,   239,   238,   237,   236,   234,   234,   234,   234,\n",
       "          232,   232,   231,   231,   230,   230,   230,   230,   229,\n",
       "          229,   228,   228,   226,   226,   225,   223,   223,   222,\n",
       "          220,   219,   219,   217,   217,   216,   215,   215,   215,\n",
       "          214,   214,   214,   211,   211,   211,   210,   210,   210,\n",
       "          210,   209,   207,   207,   207,   206,   206,   205,   205,\n",
       "          204,   204,   201,   201,   200,   200,   198,   198,   196,\n",
       "          195,   195,   194,   193,   193,   193,   192,   190,   189,\n",
       "          189,   188,   187,   187,   187,   187,   186,   186,   183,\n",
       "          183,   183,   180,   179,   179,   178,   178,   178,   177,\n",
       "          177,   175,   174,   174,   174,   173,   173,   172,   170,\n",
       "          169,   169,   169,   169,   168,   168,   167,   165,   163,\n",
       "          161,   160,   159,   158,   157,   156,   156,   156,   156,\n",
       "          152,   150,   150,   150,   149,   149,   149,   148,   147,\n",
       "          147,   147,   146,   146,   145,   145,   143,   142,   142,\n",
       "          141,   141,   140,   140,   140,   139,   139,   139,   138,\n",
       "          138,   137,   137,   136,   136,   135,   134,   134,   133,\n",
       "          133,   133,   131,   131,   131,   130,   129,   128,   128,\n",
       "          128,   127,   127,   127,   127,   127,   127,   127,   126,\n",
       "          126,   125,   125,   123,   122,   122,   121,   121,   120,\n",
       "          120,   117,   117,   117,   116,   116,   115,   115,   114,\n",
       "          113,   113,   113,   113,   113,   112,   112,   112,   112,\n",
       "          112,   110,   110,   109,   109,   109,   108,   106,   105,\n",
       "          103,   103,   103,   102,   101,   101,   100,    99,    98,\n",
       "           98,    98,    97,    97,    96,    96,    96,    96,    96,\n",
       "           95,    95,    94,    94,    94,    94,    94,    93,    93,\n",
       "           91,    91,    90,    90,    89,    89,    88,    88,    87,\n",
       "           87,    87,    85,    84,    84,    83,    83,    83,    80,\n",
       "           80,    80,    78,    76,    76,    75,    74,    73,    73,\n",
       "           73,    73,    72,    72,    72,    72,    71,    71,    70,\n",
       "           69,    69,    68,    68,    68,    67,    67,    66,    66,\n",
       "           66,    65,    65,    64,    64,    61,    61,    60,    60,\n",
       "           60,    60,    59,    58,    58,    57,    57,    57,    56,\n",
       "           55,    54,    52,    52,    52,    49,    49,    48,    47,\n",
       "           47,    47,    45,    44,    44,    43,    43,    42,    42,\n",
       "           41,    41,    39,    39,    39,    38,    37,    37,    36,\n",
       "           35,    35,    34,    34,    33,    32,    31,    31,    30,\n",
       "           30,    24,    24,    23,    21,    20,    19], dtype=int64),\n",
       " array([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,\n",
       "         13,  14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,\n",
       "         26,  27,  28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,\n",
       "         39,  40,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,\n",
       "         52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,  64,\n",
       "         65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,  77,\n",
       "         78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  88,  89,  90,\n",
       "         91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102, 103,\n",
       "        104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
       "        117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129,\n",
       "        130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142,\n",
       "        143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153, 154, 155,\n",
       "        156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168,\n",
       "        169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194,\n",
       "        195, 196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207,\n",
       "        208, 209, 210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220,\n",
       "        221, 222, 223, 224, 225, 226, 227, 228, 229, 230, 231, 232, 233,\n",
       "        234, 235, 236, 237, 238, 239, 240, 241, 242, 243, 244, 245, 246,\n",
       "        247, 248, 249, 250, 251, 252, 253, 254, 255, 256, 257, 258, 259,\n",
       "        260, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272,\n",
       "        273, 274, 275, 276, 277, 278, 279, 280, 281, 282, 283, 284, 285,\n",
       "        286, 287, 288, 289, 290, 291, 292, 293, 294, 295, 296, 297, 298,\n",
       "        299, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309, 310, 311,\n",
       "        312, 313, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323, 324,\n",
       "        325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336, 337,\n",
       "        338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349, 350,\n",
       "        351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362, 363,\n",
       "        364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 375, 376,\n",
       "        377, 378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389,\n",
       "        390, 391, 392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402,\n",
       "        403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415,\n",
       "        416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428,\n",
       "        429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441,\n",
       "        442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454,\n",
       "        455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 465, 466, 467,\n",
       "        468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480,\n",
       "        481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493,\n",
       "        494, 495, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506,\n",
       "        507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519,\n",
       "        520, 521, 522, 523, 524, 525, 526, 527, 528, 529, 530, 531, 532,\n",
       "        533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545,\n",
       "        546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 556, 557, 558,\n",
       "        559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 569, 570, 571,\n",
       "        572, 573, 574, 575, 576, 577, 578, 579, 580, 581, 582, 583, 584,\n",
       "        585, 586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597,\n",
       "        598, 599, 600, 601, 602, 603, 604, 605, 606, 607, 608, 609, 610,\n",
       "        611, 612, 613, 614, 615, 616, 617, 618, 619, 620, 621, 622, 623,\n",
       "        624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636,\n",
       "        637, 638, 639, 640, 641, 642, 643, 644, 645, 646, 647, 648, 649,\n",
       "        650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 661, 662,\n",
       "        663, 664, 665, 666, 667, 668, 669, 670, 671, 672, 673, 674, 675,\n",
       "        676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688,\n",
       "        689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 699, 700, 701,\n",
       "        702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714,\n",
       "        715, 716, 717, 718, 719, 720, 721, 722, 723, 724, 725, 726],\n",
       "       dtype=int64))"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_topic_sizes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_words, word_scores, topic_nums = model.get_topics()\n",
    "topics_df = pd.DataFrame(data = topic_words, columns=[f\"topic{i+1}\" for i in range(50)])\n",
    "topics_df.index = topic_nums\n",
    "topics_df.to_csv(os.path.join(DATA_PATH, \"topics.csv\"))\n",
    "# topic_words_str = [\",\".join(topic_word) for topic_word in topic_words]\n",
    "# topics = pd.DataFrame(data={'topic_words':topic_words_str, 'topic_nums':topic_nums})\n",
    "# topics.to_csv(os.path.join(DATA_PATH, \"topics.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['channel_name', 'comment_text', 'habit', 'community', 'progress',\n",
      "       'Fitness', 'Competence', 'Appearance', 'Enjoyment', 'Social',\n",
      "       'predicted_label', 'label', 'included'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "pprint(labeled.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_idx = {}\n",
    "# Competence\n",
    "comp_idx = labeled[labeled.Competence == 1].index.values + comments.shape[0]\n",
    "label_idx['competence'] = comp_idx.tolist()\n",
    "# Fitness\n",
    "fit_idx = labeled[labeled.Fitness == 1].index.values + comments.shape[0]\n",
    "label_idx['fitness'] = fit_idx.tolist()\n",
    "# Appearance\n",
    "app_idx = labeled[labeled.Appearance == 1].index.values + comments.shape[0]\n",
    "label_idx['appearance'] = app_idx.tolist()\n",
    "# Enjoyment\n",
    "enj_idx = labeled[labeled.Enjoyment == 1].index.values + comments.shape[0]\n",
    "label_idx['enjoyment'] = enj_idx.tolist()\n",
    "# Social\n",
    "#soc_idx = labeled[(labeled.Social == 1)].index.values + comments.shape[0]\n",
    "#label_idx['social'] = soc_idx.tolist()\n",
    "\n",
    "## Extra categories\n",
    "# Appreciation\n",
    "# label_idx['thanks'] = [325, 1086, 9474, 4650, 7789, 1036, 358,\n",
    "#                        223814, 316898, 610256, 301454, 212997, 356964, 223083, 294452, 59093, 344656,\n",
    "#                        315086, 344725, 44930]\n",
    "# More\n",
    "# label_idx['more'] = [300363, 371792, 628093, 301235, 44240]\n",
    "# # Great\n",
    "# label_idx['great'] = [86589,\n",
    "#                       102915, 347408, 580749, 103456, 314672, 44240]\n",
    "# Journey\n",
    "#label_idx['journey'] = [216508, 217844]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(830479, 3)\n",
      "                                             comment_text  include  \\\n",
      "321152  Why were my thighs shaking uncontrollably in a...     True   \n",
      "829509  My word of the day is strong.  It‚Äö√Ñ√¥s my birth...     True   \n",
      "24261   Is this flow save if you have 1 finger gap dia...     True   \n",
      "173481  Thank u!  I needed some TLC!  Thank u!  üßò‚Äç‚ôÄÔ∏èüßò‚Äç...     True   \n",
      "681035  I am suffering from pcod and I want to loss 15...    False   \n",
      "238205  I always find day 30 hard without Adriene's re...     True   \n",
      "162908                     I needed this today; thank you    False   \n",
      "414752  Thank you. That felt wonderful. Do so love the...     True   \n",
      "377868        and the funny thing is they never worked...    False   \n",
      "614101  Learn Yoga & Dance...\\n\\nhttps://youtube.com/s...    False   \n",
      "\n",
      "                                                processed  competence  \\\n",
      "321152      thigh,shake,uncontrollably,svasana,pose,end\\n    0.000000   \n",
      "829509  word,day,strong,birthday,set,stage,trip,sun,na...    0.000000   \n",
      "24261              flow,save,finger,gap,diastasis,recti\\n    0.000000   \n",
      "173481                         thank,u,need,tlc,thank,u\\n    0.000000   \n",
      "681035                      suffer,pcod,want,loss,month\\n    0.000000   \n",
      "238205  find,day,hard,adriene,reassuring,word,keep,loo...    0.567184   \n",
      "162908                                 need,today,thank\\n    0.000000   \n",
      "414752  thank,feel,wonderful,love,short,practice,stret...    0.542209   \n",
      "377868                                 funny,thing,work\\n    0.000000   \n",
      "614101                    learn,yoga,dance,learn yoga &\\n    0.000000   \n",
      "\n",
      "         fitness  appearance  enjoyment    social  max_score       label  \n",
      "321152  0.000000    0.370473        0.0  0.000000   0.370473  appearance  \n",
      "829509  0.000000    0.000000        0.0  0.000000   0.000000  competence  \n",
      "24261   0.000000    0.000000        0.0  0.000000   0.000000  competence  \n",
      "173481  0.000000    0.000000        0.0  0.000000   0.000000  competence  \n",
      "681035  0.000000    0.000000        0.0  0.000000   0.000000  competence  \n",
      "238205  0.000000    0.351211        0.0  0.645472   0.645472      social  \n",
      "162908  0.000000    0.000000        0.0  0.000000   0.000000  competence  \n",
      "414752  0.559681    0.000000        0.0  0.000000   0.559681     fitness  \n",
      "377868  0.000000    0.000000        0.0  0.000000   0.000000  competence  \n",
      "614101  0.000000    0.000000        0.0  0.000000   0.000000  competence  \n"
     ]
    }
   ],
   "source": [
    "comments_df = pd.DataFrame(data={'comment_text': comments.comment_text.tolist() + labeled.comment_text.tolist()})\n",
    "comments_df['include'] = length_include\n",
    "comments_df['processed'] = processed_docs\n",
    "print(comments_df.shape)\n",
    "\n",
    "# get similarity scores for each comment\n",
    "def get_similarity_scores(idx, model, num_docs=100000):\n",
    "    docs, scores, doc_ids = model.search_documents_by_documents(convert_id_l2s(idx), num_docs=num_docs, ef=400000)\n",
    "    return scores, convert_id_s2l(doc_ids)\n",
    "\n",
    "# fill the column for each label with the similarity scores using the doc ids\n",
    "def fill_column(idx, model, label):\n",
    "    scores, doc_ids = get_similarity_scores(idx, model)\n",
    "    comments_df[label] = 0.0\n",
    "    comments_df.loc[doc_ids, label] = scores\n",
    "\n",
    "# fill the columns for each label\n",
    "for label, idx in label_idx.items():\n",
    "    fill_column(idx, model, label)\n",
    "\n",
    "# extra for social\n",
    "soc_idx = (labeled[(labeled.Social == 1)].index.values + comments.shape[0]).tolist()\n",
    "soc_neg = [2+comments.shape[0], 344659]#, 216508, 344659, 212698, 307772, 301844, 212609, 317443, 215525, 67716, 810299,\n",
    "           #217844, 102735, 102912, 103336, 16240, 304445,322119,160198] \n",
    "\n",
    "docs, scores, doc_ids = model.search_documents_by_documents(doc_ids = convert_id_l2s(soc_idx), doc_ids_neg = convert_id_l2s(soc_neg), num_docs=100000)\n",
    "comments_df['social'] = 0.0\n",
    "comments_df.loc[convert_id_s2l(doc_ids), 'social'] = scores\n",
    "\n",
    "main_cols = ['competence', 'fitness', 'appearance', 'enjoyment', 'social']\n",
    "extra_cols = [] #['thanks', 'more', 'great', 'journey']\n",
    "\n",
    "# get the max score for each comment\n",
    "comments_df['max_score'] = comments_df[main_cols + extra_cols].max(axis=1)\n",
    "# get the label for each comment\n",
    "comments_df['label'] = comments_df[main_cols + extra_cols].idxmax(axis=1)\n",
    "\n",
    "pprint(comments_df.sample(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_off = 0.6\n",
    "comments_df.loc[comments_df.max_score < cut_off, 'label'] = 'other'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "enjoyment     27684\n",
      "competence    17924\n",
      "social        14611\n",
      "fitness        7771\n",
      "appearance        1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "pprint(comments_df[comments_df.max_score > cut_off].label.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score:0.79, index:829245, label index:1765\n",
      "(\"It's May, and I started this in January, but I'm still so proud of myself \"\n",
      " 'for finishing and learning to love myself along the way! Thanks for the '\n",
      " 'great yoga journey Adriene!')\n",
      "------------------\n",
      "score:0.78, index:235090, label index:-592390\n",
      "(\"Thank you Adriene. I'm feeling so strong, and greatful after todays \"\n",
      " 'practice. Love you. üòå Thank you yoga community. Have a great rest of the '\n",
      " 'day.')\n",
      "------------------\n",
      "score:0.78, index:180458, label index:-647022\n",
      "('Amazing. I love this flow! Wonderful practice today.  I am just so loving '\n",
      " 'the daily yoga, I can feel myself looking forward to the continued journey '\n",
      " 'after Jan. Thank you, Adriene!')\n",
      "------------------\n",
      "score:0.78, index:170043, label index:-657437\n",
      "('Thank you so much Adriene for sharing this amazing times for whole 30 days '\n",
      " 'again. I have started yoga with you in Jan 19. \\n'\n",
      " 'I love to find myself and how I feel here on the mat with you and everyone '\n",
      " 'else joining from everywhere. Thank you to you all üôèüèº. Love From my heart '\n",
      " 'you ü§ç')\n",
      "------------------\n",
      "score:0.78, index:268258, label index:-559222\n",
      "('Hello to all the loving souls that are on this beautiful journey. Today my '\n",
      " 'first thought was doing Yoga with Adriene. I woke up before time and could '\n",
      " 'not wait to start the practice. The ending today really touch my heart. '\n",
      " 'Words cannot express how I feel right now, thanks you so much for sharing '\n",
      " 'your heart with us Adriene üôèüèª!')\n",
      "------------------\n",
      "score:0.78, index:239287, label index:-588193\n",
      "('30 days yoga journey completedüßòüèº\\u200d‚ôÄÔ∏è.New beginning, new me, self '\n",
      " 'love...Find what feels good...NAMASTEüôèüèª...Thank you Adriene‚ù§Ô∏è')\n",
      "------------------\n",
      "score:0.78, index:268246, label index:-559234\n",
      "('Hello to everyone from this lovely community! Let this day be balanced and '\n",
      " 'full of joy ü§ç \\n'\n",
      " \"You're amazing to continue this yoga-journey till this point. I'm very proud \"\n",
      " 'of you. Send love and hugs\\n'\n",
      " 'Namaste')\n",
      "------------------\n",
      "score:0.77, index:507579, label index:-319901\n",
      "('Hi, I started this 30 day Yoga journey today. I love this first practice! '\n",
      " \"You are amazing. I'm so grateful to you that you share your gift with us. \"\n",
      " 'Have a beautiful day! RY')\n",
      "------------------\n",
      "score:0.77, index:204610, label index:-622870\n",
      "('I finished todays practice and I suddenly felt such a rush of gratitude I '\n",
      " 'was near tears. I know, through great times and challenging times yoga, the '\n",
      " 'whole community and you adriene all have my back. Namaste')\n",
      "------------------\n",
      "score:0.77, index:169856, label index:-657624\n",
      "('Today was 30 days completed!  Thank you, Adriene for the compassion and love '\n",
      " 'you put into \\n'\n",
      " 'your practice you have given us.  Thank you for the gift of community and '\n",
      " 'bringing yoga back into my life!  At the end of the practice tonight, tears '\n",
      " 'of joy`')\n",
      "------------------\n",
      "score:0.77, index:241966, label index:-585514\n",
      "('Adriene thanks so much for this 30 day yoga journey, I finished it today and '\n",
      " 'I am so proud of myself! I enjoyed every practice. You are great at what you '\n",
      " 'do, keep on being you! Thanks again.')\n",
      "------------------\n",
      "score:0.77, index:270060, label index:-557420\n",
      "('I feel awesome today and I really enjoyed this practice üòç\\n'\n",
      " \"It's such a pleasure each day to say Namaste at the same time with you \"\n",
      " 'Adriene. Thank you so much for sharing your kindness and wisdom with us. '\n",
      " 'Love you üíû')\n",
      "------------------\n",
      "score:0.76, index:245939, label index:-581541\n",
      "(\"Day 27. Today's practice was beautiful, in so many ways.\\n\"\n",
      " 'I felt very emotional today and I know part of it is also that the Breath '\n",
      " 'journey will end soon. But i also know how thankful and greatful I am for '\n",
      " 'this because for the rest of my time i will always listen to my breath and '\n",
      " 'continue with practicing yoga üôèüíó\\n'\n",
      " 'I love YWA community, Adriene and Benji!\\n'\n",
      " 'I healed today , thank you :)')\n",
      "------------------\n",
      "score:0.76, index:277356, label index:-550124\n",
      "('i full on cried so hard after today‚Äôs practice <3 tears of pure joy! i have '\n",
      " 'been telling myself for months to get back to daily yoga and i‚Äôm finally '\n",
      " 'doing it!!! 7 days!!!! i‚Äôm so happy! i am so grateful, thank you Adriene <3')\n",
      "------------------\n",
      "score:0.76, index:171145, label index:-656335\n",
      "('Thank you Adriene. I just finished my 30 day yoga journey and I adored this '\n",
      " 'last session. Only gratitude to you and this wonderful community. Namaste üôè')\n",
      "------------------\n",
      "score:0.76, index:744109, label index:-83371\n",
      "('Thank you dear Yogis for practicing with me this morning üå∏ Tana I love your '\n",
      " 'flows the feeling lasts all day into my evening practice. Thank you üôè '\n",
      " 'Namaste')\n",
      "------------------\n",
      "score:0.76, index:177981, label index:-649499\n",
      "('I love this practice. Today I was checking in with myself and decided to do '\n",
      " \"something that makes me feel good and happy. I'm grateful to say yoga is \"\n",
      " 'always the first thing that comes to mind. This was a wonderful way to end '\n",
      " 'my day. Thank you Adriene <3')\n",
      "------------------\n",
      "score:0.76, index:255261, label index:-572219\n",
      "('Thank you for this lovely practice and as usual for your very kind words! '\n",
      " \"You have taught me everything I know today about yoga and I'm so grateful \"\n",
      " 'for having this opportunity and for being a part of something bigger, '\n",
      " 'sending love, see you tomorrow, namaste <3')\n",
      "------------------\n",
      "score:0.76, index:239551, label index:-587929\n",
      "('That was truly amazing. There wasn‚Äôt a day that went by that I didn‚Äôt want '\n",
      " 'to do yoga with this community at the end of my day. I wasn‚Äôt always into it '\n",
      " 'in the beginning but I never regretted doing it. I am so grateful for this '\n",
      " 'practice and grateful for Adriene‚Äôs loving non-judgmental guidance. I '\n",
      " 'celebrate us all. Namaste.')\n",
      "------------------\n",
      "score:0.76, index:237742, label index:-589738\n",
      "('Thank you Adriene and everyone that went through this journey together, they '\n",
      " 'were an amazing 30 days of practice, for me this is the begin of a bigger '\n",
      " 'and wonderful journey finding myself, my strength, and my breath, and I hope '\n",
      " 'will be the same for everyone, lots of love from my heart to all of yours! '\n",
      " '‚ù§Ô∏è')\n",
      "------------------\n"
     ]
    }
   ],
   "source": [
    "for k,row in comments_df[comments_df.label==\"social\"].sort_values(by='social', ascending=False).iloc[:20].iterrows():\n",
    "# print social score with two decimal places\n",
    "    print(f\"score:{row.social:.2f}, index:{k}, label index:{k-comments.shape[0]}\")\n",
    "    pprint(row.comment_text)\n",
    "    # pprint(row.processed)\n",
    "    # print(length_include[k])\n",
    "    print('------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[221981, 131228, 62165, 224507, 191522, 207289, 224867, 86560, 73700]\n"
     ]
    }
   ],
   "source": [
    "pprint(soc_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>include</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Yesterday I was searching for a morning class ...</td>\n",
       "      <td>True</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>This is one of my favourites! I put it in the ...</td>\n",
       "      <td>True</td>\n",
       "      <td>25.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>thank you for always being so present in your ...</td>\n",
       "      <td>True</td>\n",
       "      <td>32.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>I was feeling so unmotivated to do my practice...</td>\n",
       "      <td>True</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>So many morning I started with this routine an...</td>\n",
       "      <td>True</td>\n",
       "      <td>38.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830455</th>\n",
       "      <td>When you said \"whisper to yourself I am strong...</td>\n",
       "      <td>True</td>\n",
       "      <td>425484.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830462</th>\n",
       "      <td>This was my favourite of all the 30 Day videos...</td>\n",
       "      <td>True</td>\n",
       "      <td>425488.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830464</th>\n",
       "      <td>Hi! I knw this is so random and unrelated but ...</td>\n",
       "      <td>True</td>\n",
       "      <td>425490.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830469</th>\n",
       "      <td>this video really changed my body flow...\\nMan...</td>\n",
       "      <td>True</td>\n",
       "      <td>425493.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>830476</th>\n",
       "      <td>Morning quite challenging in places, I know to...</td>\n",
       "      <td>True</td>\n",
       "      <td>425500.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>59798 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  include    doc_id\n",
       "16      Yesterday I was searching for a morning class ...     True       5.0\n",
       "55      This is one of my favourites! I put it in the ...     True      25.0\n",
       "73      thank you for always being so present in your ...     True      32.0\n",
       "79      I was feeling so unmotivated to do my practice...     True      33.0\n",
       "87      So many morning I started with this routine an...     True      38.0\n",
       "...                                                   ...      ...       ...\n",
       "830455  When you said \"whisper to yourself I am strong...     True  425484.0\n",
       "830462  This was my favourite of all the 30 Day videos...     True  425488.0\n",
       "830464  Hi! I knw this is so random and unrelated but ...     True  425490.0\n",
       "830469  this video really changed my body flow...\\nMan...     True  425493.0\n",
       "830476  Morning quite challenging in places, I know to...     True  425500.0\n",
       "\n",
       "[59798 rows x 3 columns]"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments2[(comments2.comment_text.str.contains(\"thank\")) & (comments2.include)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the labels to the original labels\n",
    "labeled['predicted_label'] = comments_df.loc[comments.shape[0]:, 'label'].tolist()\n",
    "labeled['included'] = comments_df.loc[comments.shape[0]:, 'include'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competence 32\n",
      "predicted_label\n",
      "other        30\n",
      "social        1\n",
      "enjoyment     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Enjoyment 18\n",
      "predicted_label\n",
      "other      15\n",
      "fitness     3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "other 1958\n",
      "predicted_label\n",
      "other         1615\n",
      "enjoyment      131\n",
      "competence     104\n",
      "social          56\n",
      "fitness         52\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Appearance 3\n",
      "predicted_label\n",
      "other    3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Fitness 20\n",
      "predicted_label\n",
      "other         16\n",
      "competence     4\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Social 20\n",
      "predicted_label\n",
      "other        18\n",
      "enjoyment     2\n",
      "Name: count, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "labeled['label'] = labeled.loc[:,['Fitness', 'Competence', 'Appearance', 'Enjoyment', 'Social']].idxmax(axis=1)\n",
    "labeled.loc[labeled.loc[:,['Fitness', 'Competence', 'Appearance', 'Enjoyment', 'Social']].max(axis=1)<0.5,'label'] = 'other'\n",
    "\n",
    "#pprint(labeled.label.value_counts())\n",
    "#pprint(labeled.predicted_label.value_counts())\n",
    "labeled_included = labeled[labeled.included]\n",
    "\n",
    "for label in labeled_included.label.unique():\n",
    "    print(label, labeled_included[labeled_included.label==label].shape[0])\n",
    "    print(labeled_included[labeled_included.label==label].predicted_label.value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>include</th>\n",
       "      <th>processed</th>\n",
       "      <th>competence</th>\n",
       "      <th>fitness</th>\n",
       "      <th>appearance</th>\n",
       "      <th>enjoyment</th>\n",
       "      <th>social</th>\n",
       "      <th>max_score</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>827516</th>\n",
       "      <td>Practiced with more than 400 people today!! Lo...</td>\n",
       "      <td>True</td>\n",
       "      <td>practice,people,today,lovely,vinyasa,expand,da...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827633</th>\n",
       "      <td>Hi Tim! I also will love to keep up with the c...</td>\n",
       "      <td>True</td>\n",
       "      <td>hi,tim,love,challenge,quarantine,bring,sense,c...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827637</th>\n",
       "      <td>I'm glad I could complete this class in the fi...</td>\n",
       "      <td>True</td>\n",
       "      <td>m,glad,complete,class,hour,upload,incredible,r...</td>\n",
       "      <td>0.584763</td>\n",
       "      <td>0.540039</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.584763</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827671</th>\n",
       "      <td>I love this community an I'm so proud to be a ...</td>\n",
       "      <td>True</td>\n",
       "      <td>love,community,m,proud,thank,adriene,help,find...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827678</th>\n",
       "      <td>love this! another long one for courage 2020.....</td>\n",
       "      <td>True</td>\n",
       "      <td>love,long,courage,gather,body,happy,mind,rest,...</td>\n",
       "      <td>0.605427</td>\n",
       "      <td>0.637968</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.656868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.656868</td>\n",
       "      <td>enjoyment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827748</th>\n",
       "      <td>Just hit the refresh button after the practice...</td>\n",
       "      <td>True</td>\n",
       "      <td>hit,refresh,button,practice,number,view,go,gue...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827882</th>\n",
       "      <td>The comment section is such a safe space... ev...</td>\n",
       "      <td>True</td>\n",
       "      <td>comment,section,safe,space,day,love,energy,lov...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827900</th>\n",
       "      <td>Yell if you're coming straight from HOME! Nama...</td>\n",
       "      <td>True</td>\n",
       "      <td>yell,come,straight,home,namaste,friend,let,con...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827940</th>\n",
       "      <td>‚Äö√Ñ√∫Tears are words that need to be shed‚Äö√Ñ√π, Pa...</td>\n",
       "      <td>True</td>\n",
       "      <td>√§√∫tear,word,need,paul,cohelho,tear,flow,eye,to...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.522600</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.522600</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>827974</th>\n",
       "      <td>\"Your precious life, your precious body ...\" A...</td>\n",
       "      <td>True</td>\n",
       "      <td>precious,life,precious,body,say,move,shift,fee...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828099</th>\n",
       "      <td>All the love to everyone in this journey.. We ...</td>\n",
       "      <td>True</td>\n",
       "      <td>love,journey,finish,week,s,great,achievement,m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828165</th>\n",
       "      <td>The light in me, honors the lights in everyone...</td>\n",
       "      <td>True</td>\n",
       "      <td>light,honor,light,yoga,revolution,past,present...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828535</th>\n",
       "      <td>I guess I kind of got lost few months ago...bu...</td>\n",
       "      <td>True</td>\n",
       "      <td>guess,kind,got,lose,month,ago,m,ready,find,too...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.403471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.403471</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828614</th>\n",
       "      <td>I found today very tough - I felt like I could...</td>\n",
       "      <td>True</td>\n",
       "      <td>find,today,tough,feel,like,not,concentrate,thi...</td>\n",
       "      <td>0.545275</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.545275</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828666</th>\n",
       "      <td>Just before I started today's practice, I saw ...</td>\n",
       "      <td>True</td>\n",
       "      <td>start,today,practice,see,news,hard,lockdown,ge...</td>\n",
       "      <td>0.542318</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.542318</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828722</th>\n",
       "      <td>564,104 views when I started, 611,485 views wh...</td>\n",
       "      <td>True</td>\n",
       "      <td>view,start,view,finish,people,world,practice,n...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828783</th>\n",
       "      <td>Thank you, loving this journey. Feeling uplift...</td>\n",
       "      <td>True</td>\n",
       "      <td>thank,love,journey,feel,uplifted,present,good,...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.528137</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.571971</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.571971</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828844</th>\n",
       "      <td>I started this session crying (grieving sucks ...</td>\n",
       "      <td>True</td>\n",
       "      <td>start,session,cry,grieve,suck,big,time,end,nic...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828909</th>\n",
       "      <td>Thank you so much for this wonderful gift that...</td>\n",
       "      <td>True</td>\n",
       "      <td>thank,wonderful,gift,valuable,truly,bless,find...</td>\n",
       "      <td>0.579485</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.579485</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828943</th>\n",
       "      <td>I've been doing my own thing for a while and i...</td>\n",
       "      <td>True</td>\n",
       "      <td>ve,thing,wonderful,come,yoga,adriene,exactly,p...</td>\n",
       "      <td>0.662314</td>\n",
       "      <td>0.637371</td>\n",
       "      <td>0.344115</td>\n",
       "      <td>0.699984</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.699984</td>\n",
       "      <td>enjoyment</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  include  \\\n",
       "827516  Practiced with more than 400 people today!! Lo...     True   \n",
       "827633  Hi Tim! I also will love to keep up with the c...     True   \n",
       "827637  I'm glad I could complete this class in the fi...     True   \n",
       "827671  I love this community an I'm so proud to be a ...     True   \n",
       "827678  love this! another long one for courage 2020.....     True   \n",
       "827748  Just hit the refresh button after the practice...     True   \n",
       "827882  The comment section is such a safe space... ev...     True   \n",
       "827900  Yell if you're coming straight from HOME! Nama...     True   \n",
       "827940  ‚Äö√Ñ√∫Tears are words that need to be shed‚Äö√Ñ√π, Pa...     True   \n",
       "827974  \"Your precious life, your precious body ...\" A...     True   \n",
       "828099  All the love to everyone in this journey.. We ...     True   \n",
       "828165  The light in me, honors the lights in everyone...     True   \n",
       "828535  I guess I kind of got lost few months ago...bu...     True   \n",
       "828614  I found today very tough - I felt like I could...     True   \n",
       "828666  Just before I started today's practice, I saw ...     True   \n",
       "828722  564,104 views when I started, 611,485 views wh...     True   \n",
       "828783  Thank you, loving this journey. Feeling uplift...     True   \n",
       "828844  I started this session crying (grieving sucks ...     True   \n",
       "828909  Thank you so much for this wonderful gift that...     True   \n",
       "828943  I've been doing my own thing for a while and i...     True   \n",
       "\n",
       "                                                processed  competence  \\\n",
       "827516  practice,people,today,lovely,vinyasa,expand,da...    0.000000   \n",
       "827633  hi,tim,love,challenge,quarantine,bring,sense,c...    0.000000   \n",
       "827637  m,glad,complete,class,hour,upload,incredible,r...    0.584763   \n",
       "827671  love,community,m,proud,thank,adriene,help,find...    0.000000   \n",
       "827678  love,long,courage,gather,body,happy,mind,rest,...    0.605427   \n",
       "827748  hit,refresh,button,practice,number,view,go,gue...    0.000000   \n",
       "827882  comment,section,safe,space,day,love,energy,lov...    0.000000   \n",
       "827900  yell,come,straight,home,namaste,friend,let,con...    0.000000   \n",
       "827940  √§√∫tear,word,need,paul,cohelho,tear,flow,eye,to...    0.000000   \n",
       "827974  precious,life,precious,body,say,move,shift,fee...    0.000000   \n",
       "828099  love,journey,finish,week,s,great,achievement,m...    0.000000   \n",
       "828165  light,honor,light,yoga,revolution,past,present...    0.000000   \n",
       "828535  guess,kind,got,lose,month,ago,m,ready,find,too...    0.000000   \n",
       "828614  find,today,tough,feel,like,not,concentrate,thi...    0.545275   \n",
       "828666  start,today,practice,see,news,hard,lockdown,ge...    0.542318   \n",
       "828722  view,start,view,finish,people,world,practice,n...    0.000000   \n",
       "828783  thank,love,journey,feel,uplifted,present,good,...    0.000000   \n",
       "828844  start,session,cry,grieve,suck,big,time,end,nic...    0.000000   \n",
       "828909  thank,wonderful,gift,valuable,truly,bless,find...    0.579485   \n",
       "828943  ve,thing,wonderful,come,yoga,adriene,exactly,p...    0.662314   \n",
       "\n",
       "         fitness  appearance  enjoyment  social  max_score      label  \n",
       "827516  0.000000    0.000000   0.000000     0.0   0.000000      other  \n",
       "827633  0.000000    0.000000   0.000000     0.0   0.000000      other  \n",
       "827637  0.540039    0.000000   0.000000     0.0   0.584763      other  \n",
       "827671  0.000000    0.000000   0.000000     0.0   0.000000      other  \n",
       "827678  0.637968    0.000000   0.656868     0.0   0.656868  enjoyment  \n",
       "827748  0.000000    0.000000   0.000000     0.0   0.000000      other  \n",
       "827882  0.000000    0.000000   0.000000     0.0   0.000000      other  \n",
       "827900  0.000000    0.000000   0.000000     0.0   0.000000      other  \n",
       "827940  0.522600    0.000000   0.000000     0.0   0.522600      other  \n",
       "827974  0.000000    0.000000   0.000000     0.0   0.000000      other  \n",
       "828099  0.000000    0.000000   0.000000     0.0   0.000000      other  \n",
       "828165  0.000000    0.000000   0.000000     0.0   0.000000      other  \n",
       "828535  0.000000    0.403471   0.000000     0.0   0.403471      other  \n",
       "828614  0.000000    0.000000   0.000000     0.0   0.545275      other  \n",
       "828666  0.000000    0.000000   0.000000     0.0   0.542318      other  \n",
       "828722  0.000000    0.000000   0.000000     0.0   0.000000      other  \n",
       "828783  0.528137    0.000000   0.571971     0.0   0.571971      other  \n",
       "828844  0.000000    0.000000   0.000000     0.0   0.000000      other  \n",
       "828909  0.000000    0.000000   0.000000     0.0   0.579485      other  \n",
       "828943  0.637371    0.344115   0.699984     0.0   0.699984  enjoyment  "
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments_df.loc[list(labeled_included[labeled_included.label==\"Social\"].index.values + comments.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[515619, 223728, 263519, 169274, 268895, 244393, 280170, 163723, 170568, 166280]\n"
     ]
    }
   ],
   "source": [
    "docs, scores, doc_ids = model.search_documents_by_documents(doc_ids = convert_id_l2s([827671]), num_docs=10, ef=400000)\n",
    "print(convert_id_s2l(doc_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['nice community mean lot thank guy',\n",
       "        'love thank congrat m member beautiful community almost ',\n",
       "        'm forever grateful community love tomorrow',\n",
       "        'grateful m keep go give pride love community',\n",
       "        'extremely grateful community thank put fun day  days',\n",
       "        'thank adriene thank community send love',\n",
       "        'definitely love know hard time lot gratitude community share thank',\n",
       "        'thank dear adriene m glad continual support namaste community',\n",
       "        'love community help immensely want thank true',\n",
       "        'thank adriene lot love beautiful community glad share space'],\n",
       "       dtype=object),\n",
       " array([0.7862143 , 0.7063819 , 0.7062247 , 0.7010541 , 0.6820182 ,\n",
       "        0.6601259 , 0.6581006 , 0.65275234, 0.65250397, 0.6490565 ],\n",
       "       dtype=float32),\n",
       " array([160753, 124158, 147512, 155127,  54461, 125965, 280602,  82565,\n",
       "        117194, 100663], dtype=int64))"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.query_documents(query = \"i love this community i appreciate everyone\", num_docs=10, return_documents=True, ef=400000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "top2vec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
