{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top2Vec Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from top2vec import Top2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_name = \"tokens_learn_universal-sentence-encoder\"\n",
    "model_name = \"lowercase_learn_doc2vec\"\n",
    "model_path = f\"E:/top2vec_{model_name}.model\"\n",
    "model = Top2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Topic Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of topics: 293\n",
      "Topic sizes: [19477  8936  8318  7694  6551  6108  6056  5364  4957  4093  3576  3475\n",
      "  3429  3356  3224  3198  3103  3054  2923  2851  2549  2363  2145  2078\n",
      "  2006  1950  1938  1902  1747  1721  1681  1678  1676  1507  1451  1373\n",
      "  1360  1329  1296  1281  1193  1190  1180  1069  1063  1053  1017  1009\n",
      "  1005  1001   996   985   936   918   911   895   891   880   866   819\n",
      "   807   802   801   799   799   797   760   701   699   699   696   683\n",
      "   655   650   648   636   634   625   595   593   591   581   575   569\n",
      "   567   564   556   552   545   545   541   536   534   528   518   505\n",
      "   503   502   493   485   469   462   458   431   431   425   420   417\n",
      "   417   410   397   394   381   366   362   358   356   356   356   339\n",
      "   331   322   319   318   317   314   309   306   302   301   296   294\n",
      "   289   287   287   281   277   267   260   259   248   247   247   244\n",
      "   243   242   237   236   233   230   230   227   227   225   225   224\n",
      "   221   220   215   214   213   208   207   207   206   206   205   205\n",
      "   203   202   200   200   191   191   190   186   178   174   173   172\n",
      "   171   171   170   169   168   166   159   159   154   154   153   152\n",
      "   151   149   148   144   141   139   136   133   132   131   130   128\n",
      "   126   123   122   122   121   120   120   119   116   116   116   115\n",
      "   114   114   111   110   109   106   106   106   104   100    98    96\n",
      "    96    95    95    94    94    94    93    90    88    87    87    87\n",
      "    84    84    83    82    81    78    78    77    75    74    71    71\n",
      "    69    69    68    66    65    64    60    57    56    56    56    53\n",
      "    49    46    41    40    38    38    36    36    35    34    34    33\n",
      "    33    31    30    29    28    27    27    27    26    23    22    22\n",
      "    21    21    19    13    12]\n",
      "Topic nums: [  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of topics: {model.get_num_topics()}\")\n",
    "\n",
    "# get topic sizes\n",
    "topic_sizes, topic_nums = model.get_topic_sizes()\n",
    "print(f\"Topic sizes: {topic_sizes}\")\n",
    "print(f\"Topic nums: {topic_nums}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: ['yoga' 'yogas' 'yogawithamit' 'yogatx' 'yogavcommunity' 'exercises'\n",
      " 'workouts' 'workout' 'stretching' 'excercises']\n",
      "Topic 1: ['workout' 'workouts' 'exercise' 'ejercicios' 'excercises' 'exercises'\n",
      " 'excercise' 'exercising' 'pilates' 'exercices']\n",
      "Topic 2: ['goodmorning' 'thankful' 'thankyou' 'obrigada' 'grazie' 'спасибо'\n",
      " 'gracias' 'morning' 'thank' 'day']\n",
      "Topic 3: ['mujhe' 'abhi' 'hai' 'bhi' 'hoga' 'hii' 'dekha' 'kya' 'yoga' 'fat']\n",
      "Topic 4: ['yoga' 'yogas' 'yogawithamit' 'yogavcommunity' 'yogatx' 'exercises'\n",
      " 'abrazo' 'exercise' 'meditative' 'inspiring']\n",
      "Topic 5: ['vids' 'videos' 'thankyou' 'video' 'видео' 'thx' 'vid' 'gracias' 'grazie'\n",
      " 'спасибо']\n",
      "Topic 6: ['class' 'classes' 'clases' 'clase' 'thankyou' 'awesome' 'cours' 'courses'\n",
      " 'lesson' 'tutorial']\n",
      "Topic 7: ['practice' 'practiced' 'practising' 'practicing' 'practise' 'practica'\n",
      " 'practised' 'practises' 'practices' 'pratice']\n",
      "Topic 8: ['day' 'todays' 'morning' 'goodmorning' 'today' 'hoy' 'mornings'\n",
      " 'afternoon' 'tomorrow' 'workday']\n",
      "Topic 9: ['sciatica' 'sciatic' 'hamstrings' 'squats' 'tendon' 'hamstring' 'knees'\n",
      " 'streching' 'osteoarthritis' 'scoliosis']\n",
      "Topic 10: ['crying' 'tears' 'heartbreaking' 'tearing' 'emotional' 'teared'\n",
      " 'heartbroken' 'cried' 'emotionally' 'depression']\n",
      "Topic 11: ['yoga' 'yogas' 'yogawithamit' 'yogavcommunity' 'yogatx' 'workouts'\n",
      " 'workout' 'challenge' 'fitness' 'exercise']\n",
      "Topic 12: ['gratitude' 'inspiring' 'inspirational' 'grateful' 'thankful' 'journey'\n",
      " 'abrazo' 'feeling' 'joyous' 'спасибо']\n",
      "Topic 13: ['sore' 'hamstrings' 'workout' 'hamstring' 'stretching' 'workouts'\n",
      " 'soreness' 'streching' 'sciatica' 'weightlifting']\n",
      "Topic 14: ['yogawithamit' 'lesley' 'yogavcommunity' 'lesleys' 'condolences' 'yogas'\n",
      " 'linda' 'yoga' 'abrazo' 'hugs']\n",
      "Topic 15: ['yoga' 'yogas' 'yogawithamit' 'yogatx' 'yogavcommunity' 'classes' 'class'\n",
      " 'clases' 'clase' 'guru']\n",
      "Topic 16: ['hips' 'stretching' 'hip' 'stretches' 'stretched' 'strech' 'streching'\n",
      " 'stretch' 'hamstrings' 'stretchy']\n",
      "Topic 17: ['awesome' 'amazing' 'awsome' 'fantastic' 'appreciated' 'excelente'\n",
      " 'wonderful' 'bravo' 'excellent' 'amaze']\n",
      "Topic 18: ['depression' 'fatigued' 'struggling' 'fatigue' 'everyday' 'stressful'\n",
      " 'depressed' 'day' 'sore' 'stressed']\n",
      "Topic 19: ['breathing' 'breath' 'breathwork' 'breathe' 'breaths' 'breathed'\n",
      " 'inhaling' 'lungs' 'asthma' 'lung']\n",
      "Topic 20: ['flow' 'flows' 'flowed' 'flowy' 'flowing' 'awsome' 'awesome' 'freestyle'\n",
      " 'fantastic' 'amazing']\n",
      "Topic 21: ['voice' 'awesome' 'tune' 'listened' 'soothing' 'singing' 'amazing'\n",
      " 'voices' 'sounded' 'soundtrack']\n",
      "Topic 22: ['tutorial' 'classes' 'clases' 'class' 'tutorials' 'thankyou' 'vids'\n",
      " 'videos' 'clase' 'awesome']\n",
      "Topic 23: ['calmed' 'calming' 'calmness' 'stressful' 'calm' 'calmer' 'stressed'\n",
      " 'calms' 'stress' 'stressing']\n",
      "Topic 24: ['stretching' 'stretches' 'strech' 'streching' 'stretch' 'stretched'\n",
      " 'stretchy' 'squats' 'streched' 'workouts']\n",
      "Topic 25: ['poses' 'pose' 'awesome' 'posture' 'awsome' 'postures' 'appreciated'\n",
      " 'amazing' 'practicing' 'thx']\n",
      "Topic 26: ['year' 'congratulations' 'obrigada' 'congrats' 'greetings' 'thankful'\n",
      " 'welcome' 'thankyou' 'welcomed' 'thank']\n",
      "Topic 27: ['meditation' 'meditations' 'meditating' 'meditated' 'meditative'\n",
      " 'meditate' 'mindfulness' 'relaxation' 'mantras' 'mantra']\n",
      "Topic 28: ['workout' 'workouts' 'stretching' 'exercises' 'exercised' 'ejercicios'\n",
      " 'exercise' 'practice' 'awesome' 'thx']\n",
      "Topic 29: ['yoga' 'yogas' 'yogawithamit' 'yogavcommunity' 'yogatx' 'flow' 'flowed'\n",
      " 'meditative' 'exercises' 'flows']\n"
     ]
    }
   ],
   "source": [
    "topic_words, word_scores, topic_nums = model.get_topics(30)\n",
    "for i,topic in enumerate(topic_words):\n",
    "    print(f\"Topic {i}: {topic[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyword\n",
    "interest = [\"fun\", \"like\", \"interesting\", \"exciting\", \"happy\"]\n",
    "competence = [\"challenge\", \"skill\", \"improve\", \"learn\"]\n",
    "appearance = [\"attractive\", \"weight\", \"appearance\"]\n",
    "fitness = [\"fit\", \"body\", \"healthy\", \"physical\", \"energy\", \"exercise\"]\n",
    "social = [\"social\", \"friend\", \"other\", \"people\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: interest\n",
      "Topic: 64 | Score: 0.3544735269099829\n",
      "Keywords: ['merci' 'gracias' 'danke' 'obrigada' 'спасибо' 'grazie' 'thankyou'\n",
      " 'thanks' 'thank' 'thx']\n",
      "\n",
      "Topic: 39 | Score: 0.2910992796044205\n",
      "Keywords: ['funny' 'hilarious' 'laughing' 'smile' 'laughed' 'laughter' 'laughs'\n",
      " 'giggle' 'hahaha' 'smiling']\n",
      "\n",
      "Topic: 250 | Score: 0.26098371436097545\n",
      "Keywords: ['sylvie' 'obrigada' 'спасибо' 'gracias' 'grazie' 'merci' 'danke'\n",
      " 'thankyou' 'thank' 'thanks']\n",
      "\n",
      "Topic: 175 | Score: 0.2580773521462656\n",
      "Keywords: ['gratitude' 'thankful' 'grateful' 'dank' 'thanking' 'спасибо' 'joyous'\n",
      " 'thank' 'thx' 'joyful']\n",
      "\n",
      "Topic: 34 | Score: 0.25804505786427057\n",
      "Keywords: ['wonderful' 'congrats' 'fantastic' 'awesome' 'thankful' 'congratulations'\n",
      " 'goodmorning' 'flow' 'amazing' 'gratefull']\n",
      "\n",
      "Topic 2: competence\n",
      "Topic: 7 | Score: 0.19887707506724006\n",
      "Keywords: ['practice' 'practiced' 'practising' 'practicing' 'practise' 'practica'\n",
      " 'practised' 'practises' 'practices' 'pratice']\n",
      "\n",
      "Topic: 50 | Score: 0.1920880943336898\n",
      "Keywords: ['challenge' 'congrats' 'year' 'challenges' 'congratulations' 'obrigada'\n",
      " 'grazie' 'thx' 'thankyou' 'hey']\n",
      "\n",
      "Topic: 72 | Score: 0.18551182600158675\n",
      "Keywords: ['beginner' 'beginners' 'videos' 'видео' 'tutorial' 'video' 'vids'\n",
      " 'tutorials' 'vid' 'novice']\n",
      "\n",
      "Topic: 141 | Score: 0.18304442844742838\n",
      "Keywords: ['charlie' 'obrigada' 'thankyou' 'спасибо' 'merci' 'gracias' 'lessons'\n",
      " 'grazie' 'lesson' 'danke']\n",
      "\n",
      "Topic: 11 | Score: 0.18073471477588768\n",
      "Keywords: ['yoga' 'yogas' 'yogawithamit' 'yogavcommunity' 'yogatx' 'workouts'\n",
      " 'workout' 'challenge' 'fitness' 'exercise']\n",
      "\n",
      "Topic 3: appearance\n",
      "Topic: 98 | Score: 0.2815706177173043\n",
      "Keywords: ['body' 'cuerpo' 'bodies' 'skinny' 'waist' 'beauty' 'thighs' 'compliments'\n",
      " 'hips' 'waight']\n",
      "\n",
      "Topic: 174 | Score: 0.2466477187383195\n",
      "Keywords: ['hermosa' 'gorgeous' 'linda' 'beautiful' 'pretty' 'beauty' 'beautifully'\n",
      " 'belle' 'lovely' 'adorable']\n",
      "\n",
      "Topic: 142 | Score: 0.22416828250173687\n",
      "Keywords: ['hermosa' 'beautiful' 'amazing' 'wonderful' 'beautifully' 'awesome'\n",
      " 'stunning' 'fantastic' 'gorgeous' 'incredible']\n",
      "\n",
      "Topic: 146 | Score: 0.22217691912080798\n",
      "Keywords: ['masculine' 'male' 'feminine' 'men' 'women' 'woman' 'female' 'guys'\n",
      " 'ladies' 'sexy']\n",
      "\n",
      "Topic: 186 | Score: 0.1939279615507764\n",
      "Keywords: ['hair' 'curl' 'curled' 'straighten' 'cute' 'linda' 'stunning' 'korte'\n",
      " 'adorable' 'shoulders']\n",
      "\n",
      "Topic 4: fitness\n",
      "Topic: 98 | Score: 0.2925567096347734\n",
      "Keywords: ['body' 'cuerpo' 'bodies' 'skinny' 'waist' 'beauty' 'thighs' 'compliments'\n",
      " 'hips' 'waight']\n",
      "\n",
      "Topic: 1 | Score: 0.2864363777815041\n",
      "Keywords: ['workout' 'workouts' 'exercise' 'ejercicios' 'excercises' 'exercises'\n",
      " 'excercise' 'exercising' 'pilates' 'exercices']\n",
      "\n",
      "Topic: 28 | Score: 0.28048470435807515\n",
      "Keywords: ['workout' 'workouts' 'stretching' 'exercises' 'exercised' 'ejercicios'\n",
      " 'exercise' 'practice' 'awesome' 'thx']\n",
      "\n",
      "Topic: 38 | Score: 0.27123325950640187\n",
      "Keywords: ['abs' 'crunches' 'workout' 'abdominal' 'workouts' 'pilates' 'abdomen'\n",
      " 'excercises' 'squats' 'weightlifting']\n",
      "\n",
      "Topic: 268 | Score: 0.2698992343614428\n",
      "Keywords: ['hormones' 'menstrual' 'pms' 'periods' 'thyroid' 'menopause' 'female'\n",
      " 'pelvic' 'pregnancy' 'imbalance']\n",
      "\n",
      "Topic 5: social\n",
      "Topic: 104 | Score: 0.1585521813515592\n",
      "Keywords: ['practiced' 'tysm' 'vids' 'practice' 'videos' 'viewers' 'video' 'amazing'\n",
      " 'practicing' 'practised']\n",
      "\n",
      "Topic: 258 | Score: 0.1445185289035506\n",
      "Keywords: ['yoga' 'yogas' 'yogawithamit' 'yogavcommunity' 'kassandra' 'yogatx'\n",
      " 'kasandra' 'fitness' 'shavasana' 'exercise']\n",
      "\n",
      "Topic: 246 | Score: 0.14352824756371146\n",
      "Keywords: ['yogawithamit' 'yogavcommunity' 'yoga' 'yogas' 'supportive' 'yogatx'\n",
      " 'thankyou' 'grazie' 'thanks' 'obrigada']\n",
      "\n",
      "Topic: 64 | Score: 0.1428358401254067\n",
      "Keywords: ['merci' 'gracias' 'danke' 'obrigada' 'спасибо' 'grazie' 'thankyou'\n",
      " 'thanks' 'thank' 'thx']\n",
      "\n",
      "Topic: 181 | Score: 0.13913592246332473\n",
      "Keywords: ['myself' 'admire' 'me' 'mich' 'attitude' 'appreciating' 'estoy'\n",
      " 'mindfulness' 'personality' 'compassion']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# topic 1: interest\n",
    "print(\"Topic 1: interest\")\n",
    "topic_words, word_scores, topic_scores, topic_nums = model.search_topics(keywords=interest, num_topics=5)\n",
    "for i, topic in enumerate(topic_nums):\n",
    "    print(f\"Topic: {topic} | Score: {topic_scores[i]}\")\n",
    "    print(f\"Keywords: {topic_words[i][:10]}\")\n",
    "    print() \n",
    "\n",
    "# topic 2: competence\n",
    "print(\"Topic 2: competence\")\n",
    "topic_words, word_scores, topic_scores, topic_nums = model.search_topics(keywords=competence, num_topics=5)\n",
    "for i, topic in enumerate(topic_nums):\n",
    "    print(f\"Topic: {topic} | Score: {topic_scores[i]}\")\n",
    "    print(f\"Keywords: {topic_words[i][:10]}\")\n",
    "    print()\n",
    "\n",
    "# topic 3: appearance\n",
    "print(\"Topic 3: appearance\")\n",
    "topic_words, word_scores, topic_scores, topic_nums = model.search_topics(keywords=appearance, num_topics=5)\n",
    "for i, topic in enumerate(topic_nums):\n",
    "    print(f\"Topic: {topic} | Score: {topic_scores[i]}\")\n",
    "    print(f\"Keywords: {topic_words[i][:10]}\")\n",
    "    print()\n",
    "\n",
    "# topic 4: fitness\n",
    "print(\"Topic 4: fitness\")\n",
    "topic_words, word_scores, topic_scores, topic_nums = model.search_topics(keywords=fitness, num_topics=5)\n",
    "for i, topic in enumerate(topic_nums):\n",
    "    print(f\"Topic: {topic} | Score: {topic_scores[i]}\")\n",
    "    print(f\"Keywords: {topic_words[i][:10]}\")\n",
    "    print()\n",
    "\n",
    "# topic 5: social\n",
    "print(\"Topic 5: social\")\n",
    "topic_words, word_scores, topic_scores, topic_nums = model.search_topics(keywords=social, num_topics=5)\n",
    "for i, topic in enumerate(topic_nums):\n",
    "    print(f\"Topic: {topic} | Score: {topic_scores[i]}\")\n",
    "    print(f\"Keywords: {topic_words[i][:10]}\")\n",
    "    print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ctopics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
